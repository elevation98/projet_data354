{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4386e9-c08f-42b3-a056-e5a4e5229a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d12ccf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL cible\n",
    "url = \"https://www.agenceecofin.com/a-la-une/recherche-article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9fb3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour scraper une seule page\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erreur lors de la récupération de la page : {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    # Sélection des éléments correspondant aux articles\n",
    "    article_blocks = soup.find_all(\"td\", class_=\"tsw\")\n",
    "    for block in article_blocks:\n",
    "        try:\n",
    "            # Récupérer le titre et le lien\n",
    "            title_tag = block.find(\"a\")\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            link = title_tag[\"href\"]\n",
    "\n",
    "            # Construire l'URL absolue de l'article si nécessaire\n",
    "            article_url = f\"https://www.agenceecofin.com{link}\"  # Modifier selon la structure réelle\n",
    "\n",
    "            # Récupérer la source et la date\n",
    "            source_tag = block.find(\"span\", class_=\"news-source\")\n",
    "            author = source_tag.get_text(strip=True) if source_tag else \"Auteur inconnu\"\n",
    "            date_tag = block.find(\"span\", class_=\"f.nsa\")\n",
    "            date = date_tag.get_text(strip=True) if date_tag else \"Date inconnue\"\n",
    "\n",
    "            # Récupérer le texte de l'article (dans la page principale)\n",
    "            text_tag = block.find(\"div\", class_=\"st\")\n",
    "            preview_text = text_tag.get_text(strip=True) if text_tag else \"Texte non disponible\"\n",
    "\n",
    "            # Suivre le lien pour extraire le contenu complet de l'article\n",
    "            full_text = scrape_article_content(article_url)\n",
    "\n",
    "            # Ajouter l'article au tableau\n",
    "            articles.append({\n",
    "                \"titre\": title,\n",
    "                \"auteur\": author,\n",
    "                \"date\": date,\n",
    "                \"texte_previsualisation\": preview_text,\n",
    "                \"texte_complet\": full_text,\n",
    "                \"lien\": article_url\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction des données : {e}\")\n",
    "            continue\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Fonction pour scraper le contenu complet d'un article\n",
    "def scrape_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erreur lors de la récupération du contenu de l'article : {url}\")\n",
    "        return \"Contenu inaccessible\"\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Sélectionner les paragraphes du contenu de l'article\n",
    "    content_div = soup.find(\"div\", class_=\"itemIntroText\")\n",
    "    if not content_div:\n",
    "        return \"Contenu non trouvé\"\n",
    "\n",
    "    paragraphs = content_div.find_all(\"p\") #class_=\"texte textarticle\"\n",
    "    full_text = \"\\n\".join([para.get_text(strip=True) for para in paragraphs])\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# Fonction pour scraper toutes les pages\n",
    "def scrape_all_page(url, max_pages=55):\n",
    "    all_articles = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping la page {page}...\")\n",
    "        urls = f\"{url}?page={page}\"  # Modifier selon la structure réelle de la pagination\n",
    "        articles = scrape_page(url)\n",
    "        if not articles:  # Arrêter si aucune donnée n'est trouvée\n",
    "            print(\"Aucune donnée trouvée, arrêt.\")\n",
    "            break\n",
    "        all_articles.extend(articles)\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca9e06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour sauvegarder les données dans un fichier JSON\n",
    "def save_to_json(data, filename=\"articles.json\"):\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Données sauvegardées dans le fichier {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde des données dans le fichier JSON : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "base_url = \"https://www.agenceecofin.com/a-la-une/recherche-article\"  # Remplacez par l'URL de la page principale\n",
    "articless = scrape_all_page(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1be86871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données sauvegardées dans le fichier articles_2.json\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder les articles dans un fichier JSON\n",
    "save_to_json(articless, \"articles_2.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
